{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8fkmK7Ex4t6"
      },
      "source": [
        "https://docs.google.com/spreadsheets/d/133q9H0chayhdxMEdC1rdlynld89cbINKHNO-qEfA0RI/edit?gid=38692135#gid=38692135"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hoqJsdgwpsTI",
        "outputId": "3e27d8e0-71cc-42e5-9ebd-c0c97fedce27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting bibtexparser\n",
            "  Downloading bibtexparser-1.4.3.tar.gz (55 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/55.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scholarly\n",
            "  Downloading scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pybliometrics\n",
            "  Downloading pybliometrics-4.4-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from bibtexparser) (3.2.5)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.13.5)\n",
            "Collecting deprecated (from scholarly)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fake-useragent (from scholarly)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting free-proxy (from scholarly)\n",
            "  Downloading free_proxy-1.1.3.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from scholarly) (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.2.1)\n",
            "Collecting selenium (from scholarly)\n",
            "  Downloading selenium-4.39.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sphinx-rtd-theme (from scholarly)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pybliometrics) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->scholarly) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->scholarly) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->scholarly) (2.8)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->scholarly) (2.0.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium->scholarly)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium->scholarly)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (1.9.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (25.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium->scholarly) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium->scholarly) (2.4.0)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium->scholarly)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium->scholarly)\n",
            "  Downloading wsproto-1.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.3)\n",
            "Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading pybliometrics-4.4-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.39.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.3.2-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: bibtexparser, free-proxy\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.3-py3-none-any.whl size=43549 sha256=32e2f66a4d37ad64253ba928baea0e9a323170dbac87cc4974814c0ec0e72663\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/7d/e9/1ff2509f13767a55df1279744adfb757f4ab94b2cbe761f56a\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for free-proxy: filename=free_proxy-1.1.3-py3-none-any.whl size=6097 sha256=61144a66fa869d6fc0e93ce970b7c6705c6d1143196ffa26fb2bc58f34df3f1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/5b/61/c93b717842c89aac55d0590076f64041825eb8d89712fca95a\n",
            "Successfully built bibtexparser free-proxy\n",
            "Installing collected packages: fuzzywuzzy, wsproto, python-docx, outcome, fake-useragent, deprecated, bibtexparser, trio, pybliometrics, free-proxy, trio-websocket, sphinxcontrib-jquery, groq, sphinx-rtd-theme, selenium, scholarly\n",
            "Successfully installed bibtexparser-1.4.3 deprecated-1.3.1 fake-useragent-2.2.0 free-proxy-1.1.3 fuzzywuzzy-0.18.0 groq-0.37.1 outcome-1.3.0.post0 pybliometrics-4.4 python-docx-1.2.0 scholarly-1.7.11 selenium-4.39.0 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1 trio-0.32.0 trio-websocket-0.12.2 wsproto-1.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              },
              "id": "5205a81e722b4b11bbf4e632b910be49"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pandas bibtexparser scholarly requests python-docx fuzzywuzzy pybliometrics groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUc9qrxLyVIt"
      },
      "source": [
        "## Complete Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cx6kA1crKbC",
        "outputId": "d7cdc6ad-38a9-4247-83a0-f212cc17984a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting xlwt\n",
            "  Downloading xlwt-1.3.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading xlwt-1.3.0-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlwt, primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.1.1 primp-0.15.0 xlwt-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl duckduckgo-search requests xlwt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "cnCCi3xP73zb",
        "outputId": "1ea20b91-59d3-4f8a-ef5f-f414acb7da1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
            "/tmp/ipython-input-558551881.py:459: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=css) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a775eac9ddc952236f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a775eac9ddc952236f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import bibtexparser\n",
        "from docx import Document\n",
        "from fuzzywuzzy import fuzz\n",
        "import time\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from groq import Groq\n",
        "from datetime import datetime\n",
        "\n",
        "# Search researchers by name\n",
        "def search_researcher_by_name(name: str, limit: int = 5):\n",
        "    \"\"\"\n",
        "    Search researcher details using Semantic Scholar API by name.\n",
        "    Returns list of author IDs and details.\n",
        "    \"\"\"\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/author/search\"\n",
        "    params = {\n",
        "        \"query\": name,\n",
        "        \"limit\": limit,\n",
        "        \"fields\": \"name,affiliations,paperCount,hIndex,url\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        authors = data.get(\"data\", [])\n",
        "        results = [\n",
        "            {\n",
        "                \"authorId\": author.get(\"authorId\"),\n",
        "                \"name\": author.get(\"name\"),\n",
        "                \"affiliations\": author.get(\"affiliations\", []),\n",
        "                \"paperCount\": author.get(\"paperCount\"),\n",
        "                \"hIndex\": author.get(\"hIndex\"),\n",
        "                \"url\": author.get(\"url\"),\n",
        "            }\n",
        "            for author in authors\n",
        "        ]\n",
        "        return results, None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return [], f\"Error fetching authors: {str(e)}\"\n",
        "\n",
        "# Safe API request\n",
        "def safe_request(url, headers, params):\n",
        "    \"\"\"Safe API call\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, params=params, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        return r.json(), None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {}, f\"Error: {str(e)}\"\n",
        "\n",
        "# Get author papers by ID\n",
        "def get_author_papers(author_id, author_name, api_key=None, limit=50, export_csv=False, export_excel=False):\n",
        "    headers = {\"x-api-key\": api_key} if api_key else {}\n",
        "\n",
        "    papers_url = f\"https://api.semanticscholar.org/graph/v1/author/{author_id}/papers\"\n",
        "    params = {\"limit\": limit, \"fields\": \"title,year,venue,citationCount,abstract,fieldsOfStudy,openAccessPdf,authors\"}\n",
        "    papers, error = safe_request(papers_url, headers, params)\n",
        "\n",
        "    if error:\n",
        "        return {\"error\": error}, None\n",
        "\n",
        "    papers = papers.get(\"data\", [])\n",
        "\n",
        "    papers_list = [\n",
        "        {\n",
        "            \"title\": paper.get(\"title\"),\n",
        "            \"year\": paper.get(\"year\"),\n",
        "            \"venue\": paper.get(\"venue\"),\n",
        "            \"citations\": paper.get(\"citationCount\"),\n",
        "            \"authors\": author_name,\n",
        "            \"abstract\": paper.get(\"abstract\"),\n",
        "            \"fields_of_study\": \",\".join(paper.get(\"fieldsOfStudy\", []) or []),\n",
        "            \"open_access_pdf\": paper.get(\"openAccessPdf\", {}).get(\"url\", None),\n",
        "            \"authors_api\": \",\".join([a.get(\"name\", \"\") for a in paper.get(\"authors\", [])])\n",
        "        }\n",
        "        for paper in papers\n",
        "    ]\n",
        "\n",
        "    papers_df = pd.DataFrame(papers_list)\n",
        "    papers_df = papers_df.sort_values(by=\"year\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "    if export_csv:\n",
        "        papers_df.to_csv(f\"{author_name}_papers.csv\", index=False)\n",
        "    if export_excel:\n",
        "        papers_df.to_excel(f\"{author_name}_papers.xlsx\", index=False)\n",
        "\n",
        "    return {\"authorId\": author_id, \"name\": author_name}, papers_df\n",
        "\n",
        "# Parse Excel for author names\n",
        "def parse_excel_for_authors(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='xlrd' if file_path.endswith('.xls') else 'openpyxl')\n",
        "        df = df.rename(columns=lambda x: x.strip().lower())\n",
        "        authors_col = next((col for col in df.columns if \"author\" in col.lower()), None)\n",
        "        if not authors_col:\n",
        "            return [], \"Excel file must contain a column with 'author' in its name\"\n",
        "        authors = df[authors_col].dropna().unique().tolist()\n",
        "        if not authors:\n",
        "            return [], \"No valid author names found in Excel file\"\n",
        "        return authors, None\n",
        "    except Exception as e:\n",
        "        return [], f\"Error parsing Excel file: {str(e)}\"\n",
        "\n",
        "# Parse Excel for publication data\n",
        "def parse_excel(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='xlrd' if file_path.endswith('.xls') else 'openpyxl')\n",
        "        df = df.rename(columns=lambda x: x.strip().lower())\n",
        "        title_col = next((col for col in df.columns if \"title\" in col.lower()), None)\n",
        "        authors_col = next((col for col in df.columns if \"author\" in col.lower()), None)\n",
        "        year_col = next((col for col in df.columns if \"year\" in col.lower()), None)\n",
        "        if not all([title_col, authors_col, year_col]):\n",
        "            raise ValueError(\"Excel file must contain columns for title, authors, and year\")\n",
        "        df_clean = df[[title_col, authors_col, year_col]].copy()\n",
        "        df_clean.columns = ['title', 'authors', 'year']\n",
        "        return df_clean, None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"Error parsing Excel file: {str(e)}\"\n",
        "\n",
        "# Parse bibtex\n",
        "def parse_bibtex(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as bib_file:\n",
        "            bib_database = bibtexparser.load(bib_file)\n",
        "        records = [\n",
        "            {\n",
        "                'title': entry.get('title', '').replace('{', '').replace('}', ''),\n",
        "                'authors': entry.get('author', '').replace('\\n', ' '),\n",
        "                'year': entry.get('year', '')\n",
        "            }\n",
        "            for entry in bib_database.entries\n",
        "        ]\n",
        "        return pd.DataFrame(records), None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"Error parsing BibTeX file: {str(e)}\"\n",
        "\n",
        "# Enrich with Semantic Scholar\n",
        "def enrich_with_semantic_scholar(df, api_key=None, progress=gr.Progress()):\n",
        "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/search/match\"\n",
        "    headers = {\"x-api-key\": api_key} if api_key else {}\n",
        "    df['citations'] = None\n",
        "    df['venue'] = None\n",
        "    df['abstract'] = None\n",
        "    df['fields_of_study'] = None\n",
        "    df['open_access_pdf'] = None\n",
        "    df['authors_api'] = None\n",
        "\n",
        "    client = Groq(api_key=\"Add your key Here\")\n",
        "\n",
        "    for i, (idx, row) in enumerate(df.iterrows()):\n",
        "        title = row['title']\n",
        "        year = row['year']\n",
        "        params = {\n",
        "            \"query\": title,\n",
        "            \"fields\": \"title,venue,citationCount,abstract,fieldsOfStudy,openAccessPdf,authors\",\n",
        "            \"year\": str(year) if pd.notna(year) else None\n",
        "        }\n",
        "        paper_found = False\n",
        "        for attempt in range(7):\n",
        "            try:\n",
        "                response = requests.get(base_url, params=params, headers=headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                if \"data\" in data and data[\"data\"]:\n",
        "                    paper = data[\"data\"][0]\n",
        "                    if fuzz.ratio(title.lower(), paper.get(\"title\", \"\").lower()) > 85:\n",
        "                        df.at[idx, 'citations'] = paper.get(\"citationCount\", None)\n",
        "                        df.at[idx, 'venue'] = paper.get(\"venue\", None)\n",
        "                        df.at[idx, 'abstract'] = paper.get(\"abstract\", None)\n",
        "                        df.at[idx, 'fields_of_study'] = \",\".join(paper.get(\"fieldsOfStudy\", []) or [])\n",
        "                        df.at[idx, 'open_access_pdf'] = paper.get(\"openAccessPdf\", {}).get(\"url\", None)\n",
        "                        df.at[idx, 'authors_api'] = \",\".join([a.get(\"name\", \"\") for a in paper.get(\"authors\", [])])\n",
        "                        paper_found = True\n",
        "                break\n",
        "            except requests.exceptions.HTTPError as e:\n",
        "                if response.status_code == 429:\n",
        "                    time.sleep(2**attempt)\n",
        "                    continue\n",
        "                break\n",
        "            except Exception:\n",
        "                break\n",
        "\n",
        "        if any(df.at[idx, col] is None for col in ['citations', 'venue', 'abstract', 'fields_of_study', 'open_access_pdf', 'authors_api']):\n",
        "            year_str = f\" published around {year}\" if pd.notna(year) else \"\"\n",
        "            prompt = f\"\"\"\n",
        "            Provide details for the research paper titled \"{title}\"{year_str} including citations, venue, abstract, fields of study, open access PDF, and authors.\n",
        "            Return the response strictly in the following JSON format without any additional text:\n",
        "            {{\n",
        "                \"citations\": <integer citation count or null>,\n",
        "                \"venue\": \"<string venue or journal name or null>\",\n",
        "                \"abstract\": \"<string abstract or null>\",\n",
        "                \"fields_of_study\": [<array of strings for fields of study or empty array>],\n",
        "                \"open_access_pdf\": \"<string URL to open access PDF or null>\",\n",
        "                \"authors_api\": [<array of author names as strings or empty array>]\n",
        "            }}\n",
        "            If information is not available, use null or empty array as appropriate.\n",
        "            \"\"\"\n",
        "            try:\n",
        "                completion = client.chat.completions.create(\n",
        "                    model=\"llama3-70b-8192\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=1024\n",
        "                )\n",
        "                response_text = completion.choices[0].message.content.strip()\n",
        "                data = json.loads(response_text)\n",
        "                if df.at[idx, 'citations'] is None:\n",
        "                    df.at[idx, 'citations'] = data.get('citations')\n",
        "                if df.at[idx, 'venue'] is None:\n",
        "                    df.at[idx, 'venue'] = data.get('venue')\n",
        "                if df.at[idx, 'abstract'] is None:\n",
        "                    df.at[idx, 'abstract'] = data.get('abstract')\n",
        "                if df.at[idx, 'fields_of_study'] is None:\n",
        "                    df.at[idx, 'fields_of_study'] = \",\".join(data.get('fields_of_study', [])) if data.get('fields_of_study') else None\n",
        "                if df.at[idx, 'open_access_pdf'] is None:\n",
        "                    df.at[idx, 'open_access_pdf'] = data.get('open_access_pdf')\n",
        "                if df.at[idx, 'authors_api'] is None:\n",
        "                    df.at[idx, 'authors_api'] = \",\".join(data.get('authors_api', [])) if data.get('authors_api') else None\n",
        "            except Exception:\n",
        "                pass\n",
        "        progress((i + 1) / len(df))\n",
        "    return df\n",
        "\n",
        "# Filter by year\n",
        "def filter_by_year(df, start_year, end_year):\n",
        "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
        "    return df[(df['year'] >= start_year) & (df['year'] <= end_year) & df['year'].notna()]\n",
        "\n",
        "# Generate year summary\n",
        "def generate_year_summary(df):\n",
        "    df['citations'] = pd.to_numeric(df['citations'], errors='coerce').fillna(0)\n",
        "    summary = df.groupby('year').agg({\n",
        "        'title': list,\n",
        "        'year': 'count',\n",
        "        'citations': 'sum'\n",
        "    }).rename(columns={'year': 'publication_count', 'citations': 'total_citations'}).reset_index()\n",
        "    return summary\n",
        "\n",
        "# Export summary to Excel\n",
        "def export_summary_to_excel(df, summary, author_name, file_path=\"summary.xlsx\"):\n",
        "    with pd.ExcelWriter(file_path) as writer:\n",
        "        df.to_excel(writer, sheet_name=\"Filtered_Publications\", index=False)\n",
        "        summary.to_excel(writer, sheet_name=\"Year_Summary\", index=False)\n",
        "\n",
        "# Export summary to Word\n",
        "def export_summary_to_word(summary, author_name, file_path=\"summary.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_heading(f'Year-wise Publication Summary for {author_name}', 0)\n",
        "    for _, row in summary.iterrows():\n",
        "        doc.add_heading(f\"Year: {row['year']} ({row['publication_count']} publications)\", level=1)\n",
        "        for title in row['title']:\n",
        "            doc.add_paragraph(f\"‚Ä¢ {title}\", style='List Bullet')\n",
        "    doc.save(file_path)\n",
        "\n",
        "# Generate plots\n",
        "def generate_plots(summary):\n",
        "    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    ax1.plot(summary['year'], summary['publication_count'], marker='o', color='#00ffcc')\n",
        "    ax1.set_title(\"Publication Count per Year\", color='white')\n",
        "    ax1.set_xlabel(\"Year\", color='white')\n",
        "    ax1.set_ylabel(\"Publication Count\", color='white')\n",
        "    ax1.grid(True, color=(1, 1, 1, 0.2))\n",
        "    ax1.set_facecolor('#1a1a2e')\n",
        "    fig1.set_facecolor('#1a1a2e')\n",
        "    ax1.tick_params(colors='white')\n",
        "\n",
        "    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "    ax2.plot(summary['year'], summary['total_citations'], marker='o', color='#ff007a')\n",
        "    ax2.set_title(\"Total Citations per Year\", color='white')\n",
        "    ax2.set_xlabel(\"Year\", color='white')\n",
        "    ax2.set_ylabel(\"Total Citations\", color='white')\n",
        "    ax2.grid(True, color=(1, 1, 1, 0.2))\n",
        "    ax2.set_facecolor('#1a1a2e')\n",
        "    fig2.set_facecolor('#1a1a2e')\n",
        "    ax2.tick_params(colors='white')\n",
        "\n",
        "    return fig1, fig2\n",
        "\n",
        "# Handle author selection for Researcher Name input\n",
        "def get_author_selection(author_list, error_message=None):\n",
        "    if error_message:\n",
        "        return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>Error: {error_message}</p>\", [], gr.update(visible=False)\n",
        "\n",
        "    if not author_list:\n",
        "        return None, None, None, None, None, None, None, \"<p style='color:#ff4d4d; font-weight:bold;'>No authors found. Please check the input and try again.</p>\", [], gr.update(visible=False)\n",
        "\n",
        "    authors_df = pd.DataFrame(author_list)\n",
        "    authors_df = authors_df[['authorId', 'name', 'affiliations', 'paperCount', 'hIndex', 'url']]\n",
        "\n",
        "    return (\n",
        "        None, None, None, None, None, None, authors_df,\n",
        "        \"<p style='color:#00ffcc; font-weight:bold;'>Please enter an Author ID from the table below</p>\",\n",
        "        author_list, gr.update(visible=True, placeholder=\"Enter Author ID (e.g., 123456)\")\n",
        "    )\n",
        "\n",
        "# Process function\n",
        "def process_function(input_type, excel_file, author_name, start_year, end_year, api_key, selected_author_id=None, author_list=None, progress=gr.Progress()):\n",
        "    try:\n",
        "        if input_type == \"Excel File\":\n",
        "            if excel_file is None:\n",
        "                raise ValueError(\"Please upload a valid Excel file (.xls or .xlsx)\")\n",
        "            author_names, error = parse_excel_for_authors(excel_file)\n",
        "            if error:\n",
        "                return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>{error}</p>\", [], gr.update(visible=False)\n",
        "            if not author_names:\n",
        "                raise ValueError(\"No valid author names found in Excel file\")\n",
        "\n",
        "            # Fetch papers for all authors in the Excel file\n",
        "            all_papers = []\n",
        "            author_info = {\"name\": \"Multiple Authors\", \"authorId\": None}\n",
        "            for name in author_names:\n",
        "                authors, error = search_researcher_by_name(name, limit=1)  # Take the first match for simplicity\n",
        "                if error:\n",
        "                    return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>{error}</p>\", [], gr.update(visible=False)\n",
        "                if authors:\n",
        "                    author = authors[0]\n",
        "                    _, papers_df = get_author_papers(author['authorId'], author['name'], api_key=api_key, limit=50)\n",
        "                    if papers_df is not None and not papers_df.empty:\n",
        "                        all_papers.append(papers_df)\n",
        "\n",
        "            # Combine papers from all authors\n",
        "            if all_papers:\n",
        "                df = pd.concat(all_papers).drop_duplicates(subset=['title']).reset_index(drop=True)\n",
        "            else:\n",
        "                df = pd.DataFrame()\n",
        "\n",
        "            # Parse Excel for additional papers\n",
        "            df_excel, error = parse_excel(excel_file)\n",
        "            if error:\n",
        "                return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>{error}</p>\", [], gr.update(visible=False)\n",
        "            df_excel = enrich_with_semantic_scholar(df_excel, api_key, progress)\n",
        "            df = pd.concat([df, df_excel]).drop_duplicates(subset=['title']).reset_index(drop=True)\n",
        "\n",
        "        elif input_type == \"Researcher Name\":\n",
        "            if not author_name:\n",
        "                raise ValueError(\"Please provide a researcher name\")\n",
        "\n",
        "            if not selected_author_id:\n",
        "                authors, error = search_researcher_by_name(author_name, limit=5)\n",
        "                if error:\n",
        "                    return get_author_selection([], error)\n",
        "                return get_author_selection(authors)\n",
        "\n",
        "            if not author_list:\n",
        "                authors, error = search_researcher_by_name(author_name, limit=5)\n",
        "                if error:\n",
        "                    return get_author_selection([], error)\n",
        "                author_list = authors\n",
        "\n",
        "            selected_author = next((a for a in author_list if a['authorId'] == selected_author_id), None)\n",
        "            if not selected_author:\n",
        "                return get_author_selection(author_list, \"Invalid author ID selected. Please choose an ID from the table.\")\n",
        "\n",
        "            author_info, df = get_author_papers(selected_author['authorId'], selected_author['name'], api_key=api_key, limit=50)\n",
        "            if df is None or df.empty:\n",
        "                raise ValueError(author_info.get(\"error\", \"No papers found for the selected author\"))\n",
        "\n",
        "            df = enrich_with_semantic_scholar(df, api_key, progress)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid input type\")\n",
        "\n",
        "        filtered_df = filter_by_year(df, start_year, end_year)\n",
        "        summary = generate_year_summary(filtered_df)\n",
        "        plot1, plot2 = generate_plots(summary)\n",
        "        timestamp = int(time.time())\n",
        "        excel_path = f\"summary_{author_info['name']}_{timestamp}.xlsx\"\n",
        "        word_path = f\"summary_{author_info['name']}_{timestamp}.docx\"\n",
        "        export_summary_to_excel(filtered_df, summary, author_info['name'], excel_path)\n",
        "        export_summary_to_word(summary, author_info['name'], word_path)\n",
        "        return (\n",
        "            filtered_df, summary, plot1, plot2, excel_path, word_path, None,\n",
        "            f\"<p style='color:#00ffcc; font-weight:bold;'>Processing complete for {author_info['name']}</p>\",\n",
        "            [], gr.update(visible=False)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>Error: {str(e)}</p>\", author_list or [], gr.update(visible=False)\n",
        "\n",
        "# Web3-inspired CSS\n",
        "css = \"\"\"\n",
        "body {\n",
        "    font-family: 'Inter', sans-serif;\n",
        "    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);\n",
        "    color: #e0e0e0;\n",
        "}\n",
        "h1 {\n",
        "    color: #00ffcc;\n",
        "    text-align: center;\n",
        "    margin-bottom: 20px;\n",
        "    text-shadow: 0 0 10px rgba(0, 255, 204, 0.5);\n",
        "}\n",
        "button {\n",
        "    background: linear-gradient(45deg, #ff007a, #00ffcc);\n",
        "    color: white;\n",
        "    border: none;\n",
        "    padding: 12px 24px;\n",
        "    font-size: 16px;\n",
        "    border-radius: 8px;\n",
        "    cursor: pointer;\n",
        "    transition: transform 0.2s, box-shadow 0.2s;\n",
        "}\n",
        "button:hover {\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 0 15px rgba(0, 255, 204, 0.5);\n",
        "}\n",
        "button:active {\n",
        "    transform: translateY(0);\n",
        "}\n",
        ".label {\n",
        "    font-weight: 600;\n",
        "    color: #00ffcc;\n",
        "    margin-bottom: 8px;\n",
        "}\n",
        ".gr-column, .gr-row {\n",
        "    background: rgba(255, 255, 255, 0.05);\n",
        "    backdrop-filter: blur(10px);\n",
        "    border-radius: 12px;\n",
        "    padding: 20px;\n",
        "    margin: 10px;\n",
        "    border: 1px solid rgba(255, 255, 255, 0.1);\n",
        "}\n",
        "input, .gr-textbox, .gr-file, .gr-slider, .gr-radio {\n",
        "    background: rgba(255, 255, 255, 0.1);\n",
        "    border: 1px solid rgba(255, 255, 255, 0.2);\n",
        "    color: #e0e0e0;\n",
        "    border-radius: 8px;\n",
        "    padding: 10px;\n",
        "}\n",
        "input:focus, .gr-textbox:focus {\n",
        "    border-color: #00ffcc;\n",
        "    box-shadow: 0 0 8px rgba(0, 255, 204, 0.3);\n",
        "}\n",
        ".gr-dataframe {\n",
        "    background: rgba(255, 255, 255, 0.05);\n",
        "    border-radius: 8px;\n",
        "    color: #e0e0e0;\n",
        "}\n",
        ".gr-dataframe table {\n",
        "    border-collapse: collapse;\n",
        "}\n",
        ".gr-dataframe th, .gr-dataframe td {\n",
        "    border: 1px solid rgba(255, 255, 255, 0.1);\n",
        "    padding: 8px;\n",
        "}\n",
        ".gr-dataframe th {\n",
        "    background: rgba(0, 255, 204, 0.1);\n",
        "    color: #00ffcc;\n",
        "}\n",
        ".gr-html {\n",
        "    font-size: 16px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.HTML(\"<h1>üåå Publication Analyzer</h1>\")\n",
        "    gr.Markdown(\"\"\"\n",
        "    **Welcome to the Future of Research Analysis**\n",
        "    Upload an Excel file (.xls or .xlsx) with publication data (must include 'title', 'authors', and 'year' columns) or select 'Researcher Name' to enter a name.\n",
        "    For Excel files, papers are fetched directly. For researcher names, select an author ID from the results.\n",
        "    Set a year range, add a Semantic Scholar API key (optional), and hit Submit to explore papers, summaries, and visualizations!\n",
        "    \"\"\", elem_classes=\"markdown\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_type = gr.Radio(\n",
        "                [\"Excel File\", \"Researcher Name\"], label=\"Input Type\", value=\"Excel File\",\n",
        "                elem_classes=\"radio\"\n",
        "            )\n",
        "            file_input = gr.File(label=\"Upload Excel File (.xls or .xlsx)\", file_types=[\".xls\", \".xlsx\"], visible=True)\n",
        "            author_input = gr.Textbox(\n",
        "                label=\"Researcher Name\", visible=False, placeholder=\"Enter full name, e.g., John Doe\"\n",
        "            )\n",
        "            api_key_input = gr.Textbox(\n",
        "                label=\"Semantic Scholar API Key (optional)\", type=\"password\",\n",
        "                placeholder=\"Enter your API key\"\n",
        "            )\n",
        "            current_year = datetime.now().year\n",
        "            start_year_input = gr.Slider(1900, current_year, value=1900, step=1, label=\"Start Year\")\n",
        "            end_year_input = gr.Slider(1900, current_year, value=current_year, step=1, label=\"End Year\")\n",
        "            author_id_input = gr.Textbox(\n",
        "                label=\"Selected Author ID\", visible=False, placeholder=\"Enter Author ID (e.g., 123456)\"\n",
        "            )\n",
        "            submit_button = gr.Button(\"Submit üöÄ\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            authors_df_output = gr.DataFrame(label=\"Available Authors\")\n",
        "            filtered_df_output = gr.DataFrame(label=\"Filtered Publications\")\n",
        "            summary_output = gr.DataFrame(label=\"Year Summary\")\n",
        "        with gr.Column():\n",
        "            plot1_output = gr.Plot(label=\"Publication Count\")\n",
        "            plot2_output = gr.Plot(label=\"Total Citations\")\n",
        "\n",
        "    with gr.Row():\n",
        "        excel_download = gr.File(label=\"Download Excel\")\n",
        "        word_download = gr.File(label=\"Download Word\")\n",
        "        status_output = gr.HTML(label=\"Status\")\n",
        "\n",
        "    author_list_state = gr.State(value=[])\n",
        "\n",
        "    def update_inputs(input_type):\n",
        "        return (\n",
        "            gr.update(visible=(input_type == \"Excel File\")),\n",
        "            gr.update(visible=(input_type == \"Researcher Name\")),\n",
        "            gr.update(visible=False)\n",
        "        )\n",
        "\n",
        "    input_type.change(\n",
        "        fn=update_inputs,\n",
        "        inputs=input_type,\n",
        "        outputs=[file_input, author_input, author_id_input]\n",
        "    )\n",
        "\n",
        "    def process_wrapper(input_type, file, author_name, api_key, start_year, end_year, selected_author_id, author_list):\n",
        "        excel_file = file.name if file else None\n",
        "        filtered_df, summary, plot1, plot2, excel_path, word_path, authors_df, status, new_author_list, author_id_visible = process_function(\n",
        "            input_type, excel_file, author_name, start_year, end_year, api_key, selected_author_id, author_list\n",
        "        )\n",
        "        return (\n",
        "            filtered_df, summary, plot1, plot2, excel_path, word_path, authors_df, status, new_author_list, author_id_visible\n",
        "        )\n",
        "\n",
        "    submit_button.click(\n",
        "        process_wrapper,\n",
        "        inputs=[input_type, file_input, author_input, api_key_input, start_year_input, end_year_input, author_id_input, author_list_state],\n",
        "        outputs=[\n",
        "            filtered_df_output, summary_output, plot1_output, plot2_output, excel_download, word_download,\n",
        "            authors_df_output, status_output, author_list_state, author_id_input\n",
        "        ]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7GZ-N5i746-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTJ-_MwHGvft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vUy0kO39GvcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas bibtexparser scholarly requests python-docx fuzzywuzzy pybliometrics groq\n",
        "\n",
        "\"\"\"## Complete Project\"\"\"\n",
        "\n",
        "!pip install pandas openpyxl duckduckgo-search requests xlwt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP6st7m7GvZh",
        "outputId": "24d42cdf-a0d1-4543-ba4f-30a1f118fb35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: bibtexparser in /usr/local/lib/python3.12/dist-packages (1.4.3)\n",
            "Requirement already satisfied: scholarly in /usr/local/lib/python3.12/dist-packages (1.7.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: pybliometrics in /usr/local/lib/python3.12/dist-packages (4.4)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.37.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from bibtexparser) (3.2.5)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.13.5)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.3.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.12/dist-packages (from scholarly) (2.2.0)\n",
            "Requirement already satisfied: free-proxy in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.1.3)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from scholarly) (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from scholarly) (1.2.1)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.39.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.12/dist-packages (from scholarly) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from scholarly) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from pybliometrics) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->scholarly) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->scholarly) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->scholarly) (2.8)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated->scholarly) (2.0.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->scholarly) (1.7.1)\n",
            "Requirement already satisfied: trio<1.0,>=0.31.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (0.32.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium->scholarly) (1.9.0)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.12/dist-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.19.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (25.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium->scholarly) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium->scholarly) (2.4.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium->scholarly) (1.3.0.post0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.12/dist-packages (from trio-websocket<1.0,>=0.12.2->selenium->scholarly) (1.3.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: xlwt in /usr/local/lib/python3.12/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import bibtexparser\n",
        "from docx import Document\n",
        "from fuzzywuzzy import fuzz\n",
        "import time\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from groq import Groq\n",
        "from datetime import datetime\n",
        "\n",
        "# Search researchers by name\n",
        "def search_researcher_by_name(name: str, limit: int = 5):\n",
        "    \"\"\"\n",
        "    Search researcher details using Semantic Scholar API by name.\n",
        "    Returns list of author IDs and details.\n",
        "    \"\"\"\n",
        "    url = \"https://api.semanticscholar.org/graph/v1/author/search\"\n",
        "    params = {\n",
        "        \"query\": name,\n",
        "        \"limit\": limit,\n",
        "        \"fields\": \"name,affiliations,paperCount,hIndex,url\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        authors = data.get(\"data\", [])\n",
        "        results = [\n",
        "            {\n",
        "                \"authorId\": author.get(\"authorId\"),\n",
        "                \"name\": author.get(\"name\"),\n",
        "                \"affiliations\": author.get(\"affiliations\", []),\n",
        "                \"paperCount\": author.get(\"paperCount\"),\n",
        "                \"hIndex\": author.get(\"hIndex\"),\n",
        "                \"url\": author.get(\"url\"),\n",
        "            }\n",
        "            for author in authors\n",
        "        ]\n",
        "        return results, None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return [], f\"Error fetching authors: {str(e)}\"\n",
        "\n",
        "# Safe API request\n",
        "def safe_request(url, headers, params):\n",
        "    \"\"\"Safe API call\"\"\"\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers, params=params, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        return r.json(), None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return {}, f\"Error: {str(e)}\"\n",
        "\n",
        "# Get author papers by ID\n",
        "def get_author_papers(author_id, author_name, api_key=None, limit=50, export_csv=False, export_excel=False):\n",
        "    headers = {\"x-api-key\": api_key} if api_key else {}\n",
        "\n",
        "    papers_url = f\"https://api.semanticscholar.org/graph/v1/author/{author_id}/papers\"\n",
        "    params = {\"limit\": limit, \"fields\": \"title,year,venue,citationCount,abstract,fieldsOfStudy,openAccessPdf,authors\"}\n",
        "    papers, error = safe_request(papers_url, headers, params)\n",
        "\n",
        "    if error:\n",
        "        return {\"error\": error}, None\n",
        "\n",
        "    papers = papers.get(\"data\", [])\n",
        "\n",
        "    papers_list = [\n",
        "        {\n",
        "            \"title\": paper.get(\"title\"),\n",
        "            \"year\": paper.get(\"year\"),\n",
        "            \"venue\": paper.get(\"venue\"),\n",
        "            \"citations\": paper.get(\"citationCount\"),\n",
        "            \"authors\": author_name,\n",
        "            \"abstract\": paper.get(\"abstract\"),\n",
        "            \"fields_of_study\": \",\".join(paper.get(\"fieldsOfStudy\", []) or []),\n",
        "            \"open_access_pdf\": paper.get(\"openAccessPdf\", {}).get(\"url\", None),\n",
        "            \"authors_api\": \",\".join([a.get(\"name\", \"\") for a in paper.get(\"authors\", [])])\n",
        "        }\n",
        "        for paper in papers\n",
        "    ]\n",
        "\n",
        "    papers_df = pd.DataFrame(papers_list)\n",
        "    papers_df = papers_df.sort_values(by=\"year\", ascending=True).reset_index(drop=True)\n",
        "\n",
        "    if export_csv:\n",
        "        papers_df.to_csv(f\"{author_name}_papers.csv\", index=False)\n",
        "    if export_excel:\n",
        "        papers_df.to_excel(f\"{author_name}_papers.xlsx\", index=False)\n",
        "\n",
        "    return {\"authorId\": author_id, \"name\": author_name}, papers_df\n",
        "\n",
        "# Parse Excel for author names\n",
        "def parse_excel_for_authors(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='xlrd' if file_path.endswith('.xls') else 'openpyxl')\n",
        "        df = df.rename(columns=lambda x: x.strip().lower())\n",
        "        authors_col = next((col for col in df.columns if \"author\" in col.lower()), None)\n",
        "        if not authors_col:\n",
        "            return [], \"Excel file must contain a column with 'author' in its name\"\n",
        "        authors = df[authors_col].dropna().unique().tolist()\n",
        "        if not authors:\n",
        "            return [], \"No valid author names found in Excel file\"\n",
        "        return authors, None\n",
        "    except Exception as e:\n",
        "        return [], f\"Error parsing Excel file: {str(e)}\"\n",
        "\n",
        "# Parse Excel for publication data\n",
        "def parse_excel(file_path):\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, engine='xlrd' if file_path.endswith('.xls') else 'openpyxl')\n",
        "        df = df.rename(columns=lambda x: x.strip().lower())\n",
        "        title_col = next((col for col in df.columns if \"title\" in col.lower()), None)\n",
        "        authors_col = next((col for col in df.columns if \"author\" in col.lower()), None)\n",
        "        year_col = next((col for col in df.columns if \"year\" in col.lower()), None)\n",
        "        if not all([title_col, authors_col, year_col]):\n",
        "            raise ValueError(\"Excel file must contain columns for title, authors, and year\")\n",
        "        df_clean = df[[title_col, authors_col, year_col]].copy()\n",
        "        df_clean.columns = ['title', 'authors', 'year']\n",
        "        return df_clean, None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"Error parsing Excel file: {str(e)}\"\n",
        "\n",
        "# Parse bibtex\n",
        "def parse_bibtex(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as bib_file:\n",
        "            bib_database = bibtexparser.load(bib_file)\n",
        "        records = [\n",
        "            {\n",
        "                'title': entry.get('title', '').replace('{', '').replace('}', ''),\n",
        "                'authors': entry.get('author', '').replace('\\n', ' '),\n",
        "                'year': entry.get('year', '')\n",
        "            }\n",
        "            for entry in bib_database.entries\n",
        "        ]\n",
        "        return pd.DataFrame(records), None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), f\"Error parsing BibTeX file: {str(e)}\"\n",
        "\n",
        "# Enrich with Semantic Scholar\n",
        "def enrich_with_semantic_scholar(df, api_key=None, progress=gr.Progress()):\n",
        "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/search/match\"\n",
        "    headers = {\"x-api-key\": api_key} if api_key else {}\n",
        "    df['citations'] = None\n",
        "    df['venue'] = None\n",
        "    df['abstract'] = None\n",
        "    df['fields_of_study'] = None\n",
        "    df['open_access_pdf'] = None\n",
        "    df['authors_api'] = None\n",
        "\n",
        "    client = Groq(api_key=\"Addd your key here\")\n",
        "\n",
        "    for i, (idx, row) in enumerate(df.iterrows()):\n",
        "        title = row['title']\n",
        "        year = row['year']\n",
        "        params = {\n",
        "            \"query\": title,\n",
        "            \"fields\": \"title,venue,citationCount,abstract,fieldsOfStudy,openAccessPdf,authors\",\n",
        "            \"year\": str(year) if pd.notna(year) else None\n",
        "        }\n",
        "        paper_found = False\n",
        "        for attempt in range(7):\n",
        "            try:\n",
        "                response = requests.get(base_url, params=params, headers=headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "                if \"data\" in data and data[\"data\"]:\n",
        "                    paper = data[\"data\"][0]\n",
        "                    if fuzz.ratio(title.lower(), paper.get(\"title\", \"\").lower()) > 85:\n",
        "                        df.at[idx, 'citations'] = paper.get(\"citationCount\", None)\n",
        "                        df.at[idx, 'venue'] = paper.get(\"venue\", None)\n",
        "                        df.at[idx, 'abstract'] = paper.get(\"abstract\", None)\n",
        "                        df.at[idx, 'fields_of_study'] = \",\".join(paper.get(\"fieldsOfStudy\", []) or [])\n",
        "                        df.at[idx, 'open_access_pdf'] = paper.get(\"openAccessPdf\", {}).get(\"url\", None)\n",
        "                        df.at[idx, 'authors_api'] = \",\".join([a.get(\"name\", \"\") for a in paper.get(\"authors\", [])])\n",
        "                        paper_found = True\n",
        "                break\n",
        "            except requests.exceptions.HTTPError as e:\n",
        "                if response.status_code == 429:\n",
        "                    time.sleep(2**attempt)\n",
        "                    continue\n",
        "                break\n",
        "            except Exception:\n",
        "                break\n",
        "\n",
        "        if any(df.at[idx, col] is None for col in ['citations', 'venue', 'abstract', 'fields_of_study', 'open_access_pdf', 'authors_api']):\n",
        "            year_str = f\" published around {year}\" if pd.notna(year) else \"\"\n",
        "            prompt = f\"\"\"\n",
        "            Provide details for the research paper titled \"{title}\"{year_str} including citations, venue, abstract, fields of study, open access PDF, and authors.\n",
        "            Return the response strictly in the following JSON format without any additional text:\n",
        "            {{\n",
        "                \"citations\": <integer citation count or null>,\n",
        "                \"venue\": \"<string venue or journal name or null>\",\n",
        "                \"abstract\": \"<string abstract or null>\",\n",
        "                \"fields_of_study\": [<array of strings for fields of study or empty array>],\n",
        "                \"open_access_pdf\": \"<string URL to open access PDF or null>\",\n",
        "                \"authors_api\": [<array of author names as strings or empty array>]\n",
        "            }}\n",
        "            If information is not available, use null or empty array as appropriate.\n",
        "            \"\"\"\n",
        "            try:\n",
        "                completion = client.chat.completions.create(\n",
        "                    model=\"llama3-70b-8192\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                    temperature=0.2,\n",
        "                    max_tokens=1024\n",
        "                )\n",
        "                response_text = completion.choices[0].message.content.strip()\n",
        "                data = json.loads(response_text)\n",
        "                if df.at[idx, 'citations'] is None:\n",
        "                    df.at[idx, 'citations'] = data.get('citations')\n",
        "                if df.at[idx, 'venue'] is None:\n",
        "                    df.at[idx, 'venue'] = data.get('venue')\n",
        "                if df.at[idx, 'abstract'] is None:\n",
        "                    df.at[idx, 'abstract'] = data.get('abstract')\n",
        "                if df.at[idx, 'fields_of_study'] is None:\n",
        "                    df.at[idx, 'fields_of_study'] = \",\".join(data.get('fields_of_study', [])) if data.get('fields_of_study') else None\n",
        "                if df.at[idx, 'open_access_pdf'] is None:\n",
        "                    df.at[idx, 'open_access_pdf'] = data.get('open_access_pdf')\n",
        "                if df.at[idx, 'authors_api'] is None:\n",
        "                    df.at[idx, 'authors_api'] = \",\".join(data.get('authors_api', [])) if data.get('authors_api') else None\n",
        "            except Exception:\n",
        "                pass\n",
        "        progress((i + 1) / len(df))\n",
        "    return df\n",
        "\n",
        "# Filter by year\n",
        "def filter_by_year(df, start_year, end_year):\n",
        "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
        "    return df[(df['year'] >= start_year) & (df['year'] <= end_year) & df['year'].notna()]\n",
        "\n",
        "# Generate year summary\n",
        "def generate_year_summary(df):\n",
        "    df['citations'] = pd.to_numeric(df['citations'], errors='coerce').fillna(0)\n",
        "    summary = df.groupby('year').agg({\n",
        "        'title': list,\n",
        "        'year': 'count',\n",
        "        'citations': 'sum'\n",
        "    }).rename(columns={'year': 'publication_count', 'citations': 'total_citations'}).reset_index()\n",
        "    return summary\n",
        "\n",
        "# Export summary to Excel\n",
        "def export_summary_to_excel(df, summary, author_name, file_path=\"summary.xlsx\"):\n",
        "    with pd.ExcelWriter(file_path) as writer:\n",
        "        df.to_excel(writer, sheet_name=\"Filtered_Publications\", index=False)\n",
        "        summary.to_excel(writer, sheet_name=\"Year_Summary\", index=False)\n",
        "\n",
        "# Export summary to Word\n",
        "def export_summary_to_word(summary, author_name, file_path=\"summary.docx\"):\n",
        "    doc = Document()\n",
        "    doc.add_heading(f'Year-wise Publication Summary for {author_name}', 0)\n",
        "    for _, row in summary.iterrows():\n",
        "        doc.add_heading(f\"Year: {row['year']} ({row['publication_count']} publications)\", level=1)\n",
        "        for title in row['title']:\n",
        "            doc.add_paragraph(f\"‚Ä¢ {title}\", style='List Bullet')\n",
        "    doc.save(file_path)\n",
        "\n",
        "# Generate plots\n",
        "def generate_plots(summary):\n",
        "    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    ax1.plot(summary['year'], summary['publication_count'], marker='o', color='#00ffcc')\n",
        "    ax1.set_title(\"Publication Count per Year\", color='white')\n",
        "    ax1.set_xlabel(\"Year\", color='white')\n",
        "    ax1.set_ylabel(\"Publication Count\", color='white')\n",
        "    ax1.grid(True, color=(1, 1, 1, 0.2))\n",
        "    ax1.set_facecolor('#1a1a2e')\n",
        "    fig1.set_facecolor('#1a1a2e')\n",
        "    ax1.tick_params(colors='white')\n",
        "\n",
        "    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "    ax2.plot(summary['year'], summary['total_citations'], marker='o', color='#ff007a')\n",
        "    ax2.set_title(\"Total Citations per Year\", color='white')\n",
        "    ax2.set_xlabel(\"Year\", color='white')\n",
        "    ax2.set_ylabel(\"Total Citations\", color='white')\n",
        "    ax2.grid(True, color=(1, 1, 1, 0.2))\n",
        "    ax2.set_facecolor('#1a1a2e')\n",
        "    fig2.set_facecolor('#1a1a2e')\n",
        "    ax2.tick_params(colors='white')\n",
        "\n",
        "    return fig1, fig2\n",
        "\n",
        "# Handle author selection for Researcher Name input\n",
        "def get_author_selection(author_list, error_message=None):\n",
        "    if error_message:\n",
        "        return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>Error: {error_message}</p>\", [], gr.update(visible=False)\n",
        "\n",
        "    if not author_list:\n",
        "        return None, None, None, None, None, None, None, \"<p style='color:#ff4d4d; font-weight:bold;'>No authors found. Please check the input and try again.</p>\", [], gr.update(visible=False)\n",
        "\n",
        "    authors_df = pd.DataFrame(author_list)\n",
        "    authors_df = authors_df[['authorId', 'name', 'affiliations', 'paperCount', 'hIndex', 'url']]\n",
        "\n",
        "    return (\n",
        "        None, None, None, None, None, None, authors_df,\n",
        "        \"<p style='color:#00ffcc; font-weight:bold;'>Please enter an Author ID from the table below</p>\",\n",
        "        author_list, gr.update(visible=True, placeholder=\"Enter Author ID (e.g., 123456)\")\n",
        "    )\n",
        "\n",
        "# Process function\n",
        "def process_function(input_type, excel_file, author_name, start_year, end_year, api_key, selected_author_id=None, author_list=None, progress=gr.Progress()):\n",
        "    try:\n",
        "        if input_type == \"Excel File\":\n",
        "            if excel_file is None:\n",
        "                raise ValueError(\"Please upload a valid Excel file (.xls or .xlsx)\")\n",
        "            author_names, error = parse_excel_for_authors(excel_file)\n",
        "            if error:\n",
        "                return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>{error}</p>\", [], gr.update(visible=False)\n",
        "            if not author_names:\n",
        "                raise ValueError(\"No valid author names found in Excel file\")\n",
        "\n",
        "            # Fetch papers for all authors in the Excel file\n",
        "            all_papers = []\n",
        "            author_info = {\"name\": \"Multiple Authors\", \"authorId\": None}\n",
        "            for name in author_names:\n",
        "                authors, error = search_researcher_by_name(name, limit=1)  # Take the first match for simplicity\n",
        "                if error:\n",
        "                    return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>{error}</p>\", [], gr.update(visible=False)\n",
        "                if authors:\n",
        "                    author = authors[0]\n",
        "                    _, papers_df = get_author_papers(author['authorId'], author['name'], api_key=api_key, limit=50)\n",
        "                    if papers_df is not None and not papers_df.empty:\n",
        "                        all_papers.append(papers_df)\n",
        "\n",
        "            # Combine papers from all authors\n",
        "            if all_papers:\n",
        "                df = pd.concat(all_papers).drop_duplicates(subset=['title']).reset_index(drop=True)\n",
        "            else:\n",
        "                df = pd.DataFrame()\n",
        "\n",
        "            # Parse Excel for additional papers\n",
        "            df_excel, error = parse_excel(excel_file)\n",
        "            if error:\n",
        "                return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>{error}</p>\", [], gr.update(visible=False)\n",
        "            df_excel = enrich_with_semantic_scholar(df_excel, api_key, progress)\n",
        "            df = pd.concat([df, df_excel]).drop_duplicates(subset=['title']).reset_index(drop=True)\n",
        "\n",
        "        elif input_type == \"Researcher Name\":\n",
        "            if not author_name:\n",
        "                raise ValueError(\"Please provide a researcher name\")\n",
        "\n",
        "            if not selected_author_id:\n",
        "                authors, error = search_researcher_by_name(author_name, limit=5)\n",
        "                if error:\n",
        "                    return get_author_selection([], error)\n",
        "                return get_author_selection(authors)\n",
        "\n",
        "            if not author_list:\n",
        "                authors, error = search_researcher_by_name(author_name, limit=5)\n",
        "                if error:\n",
        "                    return get_author_selection([], error)\n",
        "                author_list = authors\n",
        "\n",
        "            selected_author = next((a for a in author_list if a['authorId'] == selected_author_id), None)\n",
        "            if not selected_author:\n",
        "                return get_author_selection(author_list, \"Invalid author ID selected. Please choose an ID from the table.\")\n",
        "\n",
        "            author_info, df = get_author_papers(selected_author['authorId'], selected_author['name'], api_key=api_key, limit=50)\n",
        "            if df is None or df.empty:\n",
        "                raise ValueError(author_info.get(\"error\", \"No papers found for the selected author\"))\n",
        "\n",
        "            df = enrich_with_semantic_scholar(df, api_key, progress)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid input type\")\n",
        "\n",
        "        filtered_df = filter_by_year(df, start_year, end_year)\n",
        "        summary = generate_year_summary(filtered_df)\n",
        "        plot1, plot2 = generate_plots(summary)\n",
        "        timestamp = int(time.time())\n",
        "        excel_path = f\"summary_{author_info['name']}_{timestamp}.xlsx\"\n",
        "        word_path = f\"summary_{author_info['name']}_{timestamp}.docx\"\n",
        "        export_summary_to_excel(filtered_df, summary, author_info['name'], excel_path)\n",
        "        export_summary_to_word(summary, author_info['name'], word_path)\n",
        "        return (\n",
        "            filtered_df, summary, plot1, plot2, excel_path, word_path, None,\n",
        "            f\"<p style='color:#00ffcc; font-weight:bold;'>Processing complete for {author_info['name']}</p>\",\n",
        "            [], gr.update(visible=False)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return None, None, None, None, None, None, None, f\"<p style='color:#ff4d4d; font-weight:bold;'>Error: {str(e)}</p>\", author_list or [], gr.update(visible=False)\n",
        "\n",
        "# Web3-inspired CSS\n",
        "css = \"\"\"\n",
        "body {\n",
        "    font-family: 'Inter', sans-serif;\n",
        "    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);\n",
        "    color: #e0e0e0;\n",
        "}\n",
        "h1 {\n",
        "    color: #00ffcc;\n",
        "    text-align: center;\n",
        "    margin-bottom: 20px;\n",
        "    text-shadow: 0 0 10px rgba(0, 255, 204, 0.5);\n",
        "}\n",
        "button {\n",
        "    background: linear-gradient(45deg, #ff007a, #00ffcc);\n",
        "    color: white;\n",
        "    border: none;\n",
        "    padding: 12px 24px;\n",
        "    font-size: 16px;\n",
        "    border-radius: 8px;\n",
        "    cursor: pointer;\n",
        "    transition: transform 0.2s, box-shadow 0.2s;\n",
        "}\n",
        "button:hover {\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 0 15px rgba(0, 255, 204, 0.5);\n",
        "}\n",
        "button:active {\n",
        "    transform: translateY(0);\n",
        "}\n",
        ".label {\n",
        "    font-weight: 600;\n",
        "    color: #00ffcc;\n",
        "    margin-bottom: 8px;\n",
        "}\n",
        ".gr-column, .gr-row {\n",
        "    background: rgba(255, 255, 255, 0.05);\n",
        "    backdrop-filter: blur(10px);\n",
        "    border-radius: 12px;\n",
        "    padding: 20px;\n",
        "    margin: 10px;\n",
        "    border: 1px solid rgba(255, 255, 255, 0.1);\n",
        "}\n",
        "input, .gr-textbox, .gr-file, .gr-slider, .gr-radio {\n",
        "    background: rgba(255, 255, 255, 0.1);\n",
        "    border: 1px solid rgba(255, 255, 255, 0.2);\n",
        "    color: #e0e0e0;\n",
        "    border-radius: 8px;\n",
        "    padding: 10px;\n",
        "}\n",
        "input:focus, .gr-textbox:focus {\n",
        "    border-color: #00ffcc;\n",
        "    box-shadow: 0 0 8px rgba(0, 255, 204, 0.3);\n",
        "}\n",
        ".gr-dataframe {\n",
        "    background: rgba(255, 255, 255, 0.05);\n",
        "    border-radius: 8px;\n",
        "    color: #e0e0e0;\n",
        "}\n",
        ".gr-dataframe table {\n",
        "    border-collapse: collapse;\n",
        "}\n",
        ".gr-dataframe th, .gr-dataframe td {\n",
        "    border: 1px solid rgba(255, 255, 255, 0.1);\n",
        "    padding: 8px;\n",
        "}\n",
        ".gr-dataframe th {\n",
        "    background: rgba(0, 255, 204, 0.1);\n",
        "    color: #00ffcc;\n",
        "}\n",
        ".gr-html {\n",
        "    font-size: 16px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    gr.HTML(\"<h1>üåå Publication Analyzer</h1>\")\n",
        "    gr.Markdown(\"\"\"\n",
        "    **Welcome to the Future of Research Analysis**\n",
        "    Upload an Excel file (.xls or .xlsx) with publication data (must include 'title', 'authors', and 'year' columns) or select 'Researcher Name' to enter a name.\n",
        "    For Excel files, papers are fetched directly. For researcher names, select an author ID from the results.\n",
        "    Set a year range, add a Semantic Scholar API key (optional), and hit Submit to explore papers, summaries, and visualizations!\n",
        "    \"\"\", elem_classes=\"markdown\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_type = gr.Radio(\n",
        "                [\"Excel File\", \"Researcher Name\"], label=\"Input Type\", value=\"Excel File\",\n",
        "                elem_classes=\"radio\"\n",
        "            )\n",
        "            file_input = gr.File(label=\"Upload Excel File (.xls or .xlsx)\", file_types=[\".xls\", \".xlsx\"], visible=True)\n",
        "            author_input = gr.Textbox(\n",
        "                label=\"Researcher Name\", visible=False, placeholder=\"Enter full name, e.g., John Doe\"\n",
        "            )\n",
        "            api_key_input = gr.Textbox(\n",
        "                label=\"Semantic Scholar API Key (optional)\", type=\"password\",\n",
        "                placeholder=\"Enter your API key\"\n",
        "            )\n",
        "            current_year = datetime.now().year\n",
        "            start_year_input = gr.Slider(1900, current_year, value=1900, step=1, label=\"Start Year\")\n",
        "            end_year_input = gr.Slider(1900, current_year, value=current_year, step=1, label=\"End Year\")\n",
        "            author_id_input = gr.Textbox(\n",
        "                label=\"Selected Author ID\", visible=False, placeholder=\"Enter Author ID (e.g., 123456)\"\n",
        "            )\n",
        "            submit_button = gr.Button(\"Submit üöÄ\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            authors_df_output = gr.DataFrame(label=\"Available Authors\")\n",
        "            filtered_df_output = gr.DataFrame(label=\"Filtered Publications\")\n",
        "            summary_output = gr.DataFrame(label=\"Year Summary\")\n",
        "        with gr.Column():\n",
        "            plot1_output = gr.Plot(label=\"Publication Count\")\n",
        "            plot2_output = gr.Plot(label=\"Total Citations\")\n",
        "\n",
        "    with gr.Row():\n",
        "        excel_download = gr.File(label=\"Download Excel\")\n",
        "        word_download = gr.File(label=\"Download Word\")\n",
        "        status_output = gr.HTML(label=\"Status\")\n",
        "\n",
        "    author_list_state = gr.State(value=[])\n",
        "\n",
        "    def update_inputs(input_type):\n",
        "        return (\n",
        "            gr.update(visible=(input_type == \"Excel File\")),\n",
        "            gr.update(visible=(input_type == \"Researcher Name\")),\n",
        "            gr.update(visible=False)\n",
        "        )\n",
        "\n",
        "    input_type.change(\n",
        "        fn=update_inputs,\n",
        "        inputs=input_type,\n",
        "        outputs=[file_input, author_input, author_id_input]\n",
        "    )\n",
        "\n",
        "    def process_wrapper(input_type, file, author_name, api_key, start_year, end_year, selected_author_id, author_list):\n",
        "        excel_file = file.name if file else None\n",
        "        filtered_df, summary, plot1, plot2, excel_path, word_path, authors_df, status, new_author_list, author_id_visible = process_function(\n",
        "            input_type, excel_file, author_name, start_year, end_year, api_key, selected_author_id, author_list\n",
        "        )\n",
        "        return (\n",
        "            filtered_df, summary, plot1, plot2, excel_path, word_path, authors_df, status, new_author_list, author_id_visible\n",
        "        )\n",
        "\n",
        "    submit_button.click(\n",
        "        process_wrapper,\n",
        "        inputs=[input_type, file_input, author_input, api_key_input, start_year_input, end_year_input, author_id_input, author_list_state],\n",
        "        outputs=[\n",
        "            filtered_df_output, summary_output, plot1_output, plot2_output, excel_download, word_download,\n",
        "            authors_df_output, status_output, author_list_state, author_id_input\n",
        "        ]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "n4zyhBvVGvWs",
        "outputId": "2afc4b92-2458-4c74-c32b-ecf3fd88cce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1612452255.py:459: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(css=css) as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f7f61a01e13cf2ca1f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f7f61a01e13cf2ca1f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}